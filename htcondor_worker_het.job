#!/bin/bash 
#SBATCH -A dasrepo 
#SBATCH -q debug 
#SBATCH -t 2
#SBATCH -N 2 -C cpu
#SBATCH hetjob
#SBATCH -N 2 -C gpu -G 4

#SBATCH --job-name=htcondor_worker_het
#SBATCH --exclusive

#SBATCH -e /global/homes/t/tylern/htcondor_workflow_scron/logs/worker_%j.err
#SBATCH -o /global/homes/t/tylern/htcondor_workflow_scron/logs/worker_%j.out


function killall {
    echo $(date)": Stopping workers"
    kill $(ps aux | grep -v grep | grep -i condor | awk '{print $2}')
    echo $(date)": Done!"
}

trap killall EXIT

# Move to the correct folder
cd ${HOME}/htcondor_workflow_scron

# For each node start a worker
echo $(date)": Starting Nodes "

export CONDOR_INSTALL=/global/common/software/m3792/htcondor-9.11.2
export PATH=$CONDOR_INSTALL/bin:$CONDOR_INSTALL/sbin:$PATH
export LOGDIR=${SCRATCH}/htcondorscratch
export CONDOR_PORT=9876
export CONDOR_SERVER=$(cat ${LOGDIR}/currenthost)

export CONDOR_CONFIG=${HOME}/htcondor_workflow_scron/htcondor_worker.conf
echo $CONDOR_CONFIG

for node in $(scontrol show hostnames ${SLURM_JOB_NODELIST_PACK_GROUP_0}); do
    echo $node
    rm -rf ${LOGDIR}/$node

    mkdir -p ${LOGDIR}/$node/log
    mkdir -p ${LOGDIR}/$node/execute
    mkdir -p ${LOGDIR}/$node/spool
    srun --het-group=0 -N 1 -n 1 -c 1 condor_master -f &
done;

for node in $(scontrol show hostnames ${SLURM_JOB_NODELIST_PACK_GROUP_1}); do
    echo $node
    rm -rf ${LOGDIR}/$node

    mkdir -p ${LOGDIR}/$node/log
    mkdir -p ${LOGDIR}/$node/execute
    mkdir -p ${LOGDIR}/$node/spool
    srun --het-group=1 -N 1 -n 1 -c 1 condor_master -f &
done;


sleep infinity
